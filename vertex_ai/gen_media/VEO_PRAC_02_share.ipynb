{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95cba2ec-d6e6-4c79-935c-a26a902fd66a",
   "metadata": {},
   "source": [
    "## CE Enablement Session - Practice 02\n",
    "- ## Workflow: Story telling: IMG --> VEO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0034fcc8-e2ed-4ad2-96bf-b1e8f240c788",
   "metadata": {},
   "source": [
    "### 1. (권장) 가상환경 생성 및 활성화\n",
    "python -m venv venv\n",
    "#### macOS/Linux:\n",
    "source venv/bin/activate\n",
    "\n",
    "#### 2. 필수 파이썬 라이브러리 설치\n",
    "pip install google-cloud-aiplatform google-cloud-texttospeech moviepy Pillow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "765d6455-dbb3-4cca-b05b-60668b307dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API 클라이언트를 초기화합니다...\n",
      "✅ API 클라이언트 초기화 완료.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. 패키지 IMPORT\n",
    "# ==============================================================================\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import base64\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Google Cloud & GenAI\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import requests\n",
    "from google.cloud import texttospeech\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "# Media Processing (MoviePy & PIL)\n",
    "from moviepy.editor import (\n",
    "    VideoFileClip, AudioFileClip, CompositeVideoClip,\n",
    "    concatenate_videoclips, ImageClip, CompositeAudioClip\n",
    ")\n",
    "import moviepy.video.fx.all as vfx\n",
    "import moviepy.audio.fx.all as afx\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 전역 설정 (CONFIGURATION)\n",
    "# ==============================================================================\n",
    "# --- Google Cloud & Vertex AI 설정 ---\n",
    "PROJECT_ID = \"jc-gcp-project\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "# --- 모델 이름 설정 ---\n",
    "STORY_MODEL = \"gemini-2.5-flash\"\n",
    "IMAGEN_MODEL = \"imagen-4.0-generate-preview-06-06\"\n",
    "VEO_MODEL = \"veo-3.0-generate-preview\"\n",
    "LYRIA_MODEL_ENDPOINT = (\n",
    "    f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}\"\n",
    "    f\"/locations/us-central1/publishers/google/models/lyria-002:predict\"\n",
    ")\n",
    "\n",
    "# --- TTS 목소리 설정 ---\n",
    "TTS_VOICE_NAME = \"ko-KR-Chirp3-HD-Callirrhoe\" # https://cloud.google.com/text-to-speech/docs/list-voices-and-types\n",
    "\n",
    "\n",
    "# --- 출력 디렉토리 설정 ---\n",
    "OUTPUT_DIR = \"veo_story_telling\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. API 클라이언트 초기화 (CLIENT INITIALIZATION)\n",
    "# ==============================================================================\n",
    "# 각 API 클라이언트를 처음에 한 번만 생성하여 재사용합니다.\n",
    "print(\"API 클라이언트를 초기화합니다...\")\n",
    "genai_client = genai.Client(vertexai=True, project=PROJECT_ID, location=\"global\")\n",
    "tts_client = texttospeech.TextToSpeechClient()\n",
    "print(\"✅ API 클라이언트 초기화 완료.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. 기능별 함수 정의 (FUNCTIONS)\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_storyline(\n",
    "    client: genai.Client,\n",
    "    video_description: str,\n",
    "    min_scenes: int = 1\n",
    ") -> Dict:\n",
    "    \"\"\"1단계: Gemini를 사용하여 비디오 스토리라인을 생성합니다.\"\"\"\n",
    "    print(\"🎬 1단계: 스토리라인 생성 중...\")\n",
    "    \n",
    "    # 함수 내에서 클라이언트를 생성하는 대신, 파라미터로 전달받아 사용합니다.\n",
    "    model = STORY_MODEL\n",
    "    \n",
    "    storyline_schema = {\n",
    "    \"type\": \"OBJECT\",\n",
    "    \"properties\": {\n",
    "        \"description\": {\"type\": \"STRING\", \"description\": \"Summary of the video\"},\n",
    "        \"music\": {\"type\": \"STRING\", \"description\": \"Description of music style (15 words or less)\"},\n",
    "        \"scenes\": {\n",
    "            \"type\": \"ARRAY\",\n",
    "            \"description\": f\"An array of at least {min_scenes} scenes for the video.\",\n",
    "            \"items\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\n",
    "                    \"story\": {\"type\": \"STRING\", \"description\": \"Storyline for this specific scene\"},\n",
    "                    \"script\": {\"type\": \"STRING\", \"description\": \"Voiceover script for this scene\"}\n",
    "                },\n",
    "                \"required\": [\"story\", \"script\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"description\", \"music\", \"scenes\"]}\n",
    "\n",
    "    prompt = f\"\"\"You are a professional video producer.\n",
    "                Create a storyline for video based on the content below.\n",
    "\n",
    "                [VIDEO_DESCRIPTION]\n",
    "                {video_description}\n",
    "\n",
    "                Please create a storyline at least {min_scenes} scenes.\n",
    "                Each scene should contain one key piece of information and flow naturally to the next scene.\n",
    "\n",
    "                Use this JSON schema for output:\n",
    "                {{\n",
    "                    \"description\": \"Summary of video\",\n",
    "                    \"music\": \"Description of music style (15 words or less)\",\n",
    "                    \"scenes\": [\n",
    "                        {{\"story\": \"Storyline for the first scene\", \"script\": \"Script for voice\"}},\n",
    "                        {{\"story\": \"Storyline for the second scene\", \"script\": \"Script for voice\"}},\n",
    "                        ...\n",
    "                    ]\n",
    "                }}\"\"\"\n",
    "    \n",
    "    contents = [types.Content(role=\"user\", parts=[types.Part.from_text(text=prompt)])]\n",
    "\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=0.95,\n",
    "        seed=0,\n",
    "        max_output_tokens=8192,\n",
    "        response_mime_type = \"application/json\",\n",
    "        response_schema=storyline_schema,\n",
    "        system_instruction=[types.Part.from_text(text=\"\"\"Script must be in KOREAN. Music must be in English\"\"\")],\n",
    "    )\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config\n",
    "    )\n",
    "    \n",
    "    clean_text = response.text.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "    storyline = json.loads(clean_text)\n",
    "    print('###### storyline 확인 하기 ######')\n",
    "    print(storyline)\n",
    "    print(f\"✅ 스토리라인 생성 완료: {len(storyline['scenes'])}개 장면\")\n",
    "    return storyline\n",
    "\n",
    "def generate_image_with_prior_context(\n",
    "    client: genai.Client,\n",
    "    scene_story: str,\n",
    "    prior_image_path: Optional[str] = None,\n",
    "    scene_idx: int = 0\n",
    ") -> str:\n",
    "    \"\"\"2단계: 이전 이미지를 참고하여 다음 장면 이미지를 생성합니다.\"\"\"\n",
    "    print(f\"🖼️  2단계: 장면 {scene_idx + 1} 이미지 생성 중...\")\n",
    "    \n",
    "    if prior_image_path and os.path.exists(prior_image_path):\n",
    "        prompt_text = f\"\"\"You are producing a video, and you want to create an image for the next scene.\n",
    "                        The previous scene ended with the image below, and now you need to create an image for this scene:\n",
    "                        [SCENE_STORY] {scene_story}\n",
    "                        Please create a prompt for image generation that references the previous image for visual continuity and creates a natural transition.\n",
    "                        Use this JSON schema for output: {{\"prompt\": \"prompt for image generation with continuity\"}}\"\"\"\n",
    "        \n",
    "        schema = {\"type\": \"OBJECT\", \"properties\": {\"prompt\": {\"type\": \"STRING\"}}, \"required\": [\"prompt\"]}\n",
    "        \n",
    "        with open(prior_image_path, \"rb\") as f:\n",
    "            image_bytes = f.read()\n",
    "        image_part = types.Part.from_bytes(data=image_bytes, mime_type=\"image/png\")\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=STORY_MODEL,\n",
    "            contents=[prompt_text, image_part],\n",
    "            config=types.GenerateContentConfig(response_mime_type=\"application/json\", response_schema=schema)\n",
    "        )\n",
    "        image_prompt = json.loads(response.text)[\"prompt\"]\n",
    "    else:\n",
    "        image_prompt = f\"Create a high-quality cinematic image for this scene: {scene_story}. The image should be suitable for video generation.\"\n",
    "\n",
    "    response_imagen = client.models.generate_images(\n",
    "        model=IMAGEN_MODEL,\n",
    "        prompt=image_prompt,\n",
    "        config=types.GenerateImagesConfig(aspect_ratio=\"16:9\", number_of_images=1),\n",
    "    )\n",
    "    \n",
    "    image_path = f\"{OUTPUT_DIR}/scene_{scene_idx}_image.png\"\n",
    "    generated_image_bytes = response_imagen.generated_images[0].image.image_bytes\n",
    "    with open(image_path, \"wb\") as f:\n",
    "        f.write(generated_image_bytes)\n",
    "\n",
    "    print(f\"✅ 장면 {scene_idx + 1} 이미지 생성 완료: {image_path}\")\n",
    "    return image_path\n",
    "\n",
    "def generate_video_from_image(\n",
    "    client: genai.Client,\n",
    "    image_path: str,\n",
    "    scene_story: str,\n",
    "    scene_idx: int\n",
    ") -> str:\n",
    "    \"\"\"3단계: 생성된 이미지로부터 VEO 비디오를 생성합니다. [Image 클래스 복원됨]\"\"\"\n",
    "    print(f\"📹 3단계: 장면 {scene_idx + 1} 비디오 생성 중...\")\n",
    "\n",
    "    # --- [수정] VEO API가 요구하는 특정 객체 구조를 위해 내부 Image 클래스를 복원합니다 ---\n",
    "    # 이 클래스는 generate_videos 함수의 image 파라미터와 호환됩니다.\n",
    "    import mimetypes\n",
    "\n",
    "    class ImageForVEO:\n",
    "        def __init__(self, gcs_uri=None, image_bytes=None, mime_type=None):\n",
    "            self.gcs_uri = gcs_uri\n",
    "            self.image_bytes = image_bytes\n",
    "            self.mime_type = mime_type\n",
    "\n",
    "    try:\n",
    "        mime_type, _ = mimetypes.guess_type(image_path)\n",
    "        if not mime_type:\n",
    "            raise ValueError(f\"Could not determine MIME type for {image_path}\")\n",
    "\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            image_byte_data = image_file.read()\n",
    "\n",
    "        # API가 요구하는 형식의 이미지 인스턴스 생성\n",
    "        image_instance = ImageForVEO(\n",
    "            gcs_uri=None,\n",
    "            image_bytes=image_byte_data,\n",
    "            mime_type=mime_type\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"Generate a video from this image that shows: {scene_story}.\n",
    "                    The video should be smooth and cinematic, lasting 3-5 seconds.\"\"\"\n",
    "        \n",
    "        operation = client.models.generate_videos(\n",
    "            model=VEO_MODEL,\n",
    "            prompt=prompt,\n",
    "            image=image_instance, # 복원된 인스턴스 사용\n",
    "        )\n",
    "\n",
    "        print(\"...VEO가 비디오를 생성하는 동안 대기합니다 (약 1~2분 소요)...\")\n",
    "        while not operation.done:\n",
    "            time.sleep(10)\n",
    "            operation = client.operations.get(operation)\n",
    "\n",
    "        if operation.error:\n",
    "            error_message = getattr(operation.error, 'message', str(operation.error))\n",
    "            raise Exception(f\"비디오 생성 실패: {error_message}\")\n",
    "\n",
    "        video_bytes = operation.response.generated_videos[0].video.video_bytes\n",
    "        video_path = f\"{OUTPUT_DIR}/scene_{scene_idx}_video.mp4\"\n",
    "        with open(video_path, 'wb') as f:\n",
    "            f.write(video_bytes)\n",
    "        \n",
    "        print(f\"✅ 장면 {scene_idx + 1} 비디오 저장 완료: '{video_path}'\")\n",
    "        return video_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 장면 {scene_idx + 1} 비디오 생성 중 오류 발생: {e}\")\n",
    "        return \"\" # 오류 발생 시 빈 경로 반환\n",
    "\n",
    "\n",
    "def generate_tts_audio(\n",
    "    client: texttospeech.TextToSpeechClient,\n",
    "    script: str,\n",
    "    scene_idx: int,\n",
    "    lang_code: str = \"ko-KR\"\n",
    ") -> str:\n",
    "    \"\"\"4단계: TTS 오디오를 생성합니다.\"\"\"\n",
    "    print(f\"🗣️  4단계: 장면 {scene_idx + 1} TTS 오디오 생성 중...\")\n",
    "    \n",
    "    synthesis_input = texttospeech.SynthesisInput(text=script)\n",
    "    voice = texttospeech.VoiceSelectionParams(language_code=lang_code, name=TTS_VOICE_NAME)\n",
    "    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "    \n",
    "    response = client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)\n",
    "    \n",
    "    audio_path = f\"{OUTPUT_DIR}/scene_{scene_idx}_audio.mp3\"\n",
    "    with open(audio_path, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "    \n",
    "    print(f\"✅ 장면 {scene_idx + 1} TTS 생성 완료: {audio_path}\")\n",
    "    return audio_path\n",
    "\n",
    "def generate_background_music(music_description: str, duration: int) -> str:\n",
    "    \"\"\"5단계: Lyria API를 사용하여 배경 음악을 생성합니다.\"\"\"\n",
    "    print(f\"🎵 5단계: 배경음악 생성 중... (설명: '{music_description}')\")\n",
    "    \n",
    "    def send_request_with_retry(data=None, max_retries=3):\n",
    "        creds, _ = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                creds.refresh(auth_req)\n",
    "                headers[\"Authorization\"] = f\"Bearer {creds.token}\"\n",
    "                response = requests.post(LYRIA_MODEL_ENDPOINT, headers=headers, json=data)\n",
    "                response.raise_for_status()\n",
    "                return response.json()\n",
    "            except HTTPError as e:\n",
    "                print(f\"❌ HTTP 에러 (시도 {attempt + 1}/{max_retries}): {e}\")\n",
    "                if e.response:\n",
    "                    print(f\"  - 서버 응답: {e.response.text}\")\n",
    "                if attempt + 1 < max_retries:\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    raise\n",
    "        raise Exception(\"API 요청 최종 실패\")\n",
    "\n",
    "    request_data = {\n",
    "      \"instances\": [{\"prompt\": music_description, \"negative_prompt\": \"dark\"}],\n",
    "      \"parameters\": {\"duration_seconds\": int(duration), \"sample_count\": 1}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        predictions = send_request_with_retry(request_data)[\"predictions\"]\n",
    "        b64_audio_data = predictions[0]['bytesBase64Encoded']\n",
    "        music_path = f\"{OUTPUT_DIR}/background_music.mp3\"\n",
    "        with open(music_path, \"wb\") as f:\n",
    "            f.write(base64.b64decode(b64_audio_data))\n",
    "        print(f\"✅ 배경음악 생성 완료: {music_path}\")\n",
    "        return music_path\n",
    "    except Exception as e:\n",
    "        print(f\"🚨 배경음악 생성 실패: {e}. 배경음악 없이 진행합니다.\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_last_frame(video_path: str) -> Optional[str]:\n",
    "    \"\"\"영상에서 마지막 프레임을 추출하여 다음 장면 생성에 사용합니다.\"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"⚠️ 마지막 프레임 추출 건너뛰기: '{video_path}' 파일 없음.\")\n",
    "        return None\n",
    "    try:\n",
    "        with VideoFileClip(video_path) as clip:\n",
    "            last_frame_time = clip.duration - (1 / clip.fps)\n",
    "            last_frame = clip.get_frame(last_frame_time)\n",
    "            \n",
    "            last_frame_image = Image.fromarray(last_frame)\n",
    "            last_frame_path = f\"{OUTPUT_DIR}/last_frame_{int(time.time())}.png\"\n",
    "            last_frame_image.save(last_frame_path)\n",
    "            \n",
    "            print(f\"✅ 마지막 프레임 추출 완료: {last_frame_path}\")\n",
    "            return last_frame_path\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 마지막 프레임 추출 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def combine_video_clips(\n",
    "    video_paths: list[str],\n",
    "    audio_paths: list[str],\n",
    "    background_music_path: str\n",
    ") -> str:\n",
    "    \"\"\"6단계: 모든 비디오, 오디오, 배경음악을 병합합니다.\"\"\"\n",
    "    print(\"🖇️  6단계: 최종 영상 병합 시작...\")\n",
    "\n",
    "    def sanitize_video_clip(video):\n",
    "        \"\"\"비디오의 실제 사용 가능한 길이를 찾아 클립을 안정화시킵니다.\"\"\"\n",
    "        try:\n",
    "            if not all([hasattr(video, 'fps'), hasattr(video, 'duration'), video.fps, video.duration]):\n",
    "                return video\n",
    "            frame_time = 1 / video.fps\n",
    "            real_duration = video.duration\n",
    "            for frame_number in reversed(range(int(video.fps * video.duration))):\n",
    "                try:\n",
    "                    current_time = frame_number * frame_time\n",
    "                    video.get_frame(current_time)\n",
    "                    real_duration = current_time + frame_time\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "            if abs(real_duration - video.duration) > frame_time:\n",
    "                print(f\"✂️ 클립 안정화: {video.duration:.2f}s -> {real_duration:.2f}s\")\n",
    "                return video.subclip(0, real_duration)\n",
    "            return video\n",
    "        except Exception as e:\n",
    "            print(f\"🚨 클립 안정화 중 오류: {e}. 원본 클립 사용.\")\n",
    "            return video\n",
    "\n",
    "    video_clips = []\n",
    "    for video_path, audio_path in zip(video_paths, audio_paths):\n",
    "        try:\n",
    "            if not os.path.exists(video_path) or os.path.getsize(video_path) < 1024:\n",
    "                print(f\"⚠️ 파일이 없거나 손상되어 건너뜁니다: {video_path}\")\n",
    "                continue\n",
    "\n",
    "            video_clip = VideoFileClip(video_path)\n",
    "            video_clip = sanitize_video_clip(video_clip)\n",
    "            audio_clip = AudioFileClip(audio_path)\n",
    "            \n",
    "            if video_clip.duration < audio_clip.duration:\n",
    "                speed_factor = video_clip.duration / audio_clip.duration\n",
    "                video_clip = vfx.speedx(video_clip, factor=speed_factor).set_duration(audio_clip.duration)\n",
    "            else:\n",
    "                video_clip = video_clip.subclip(0, audio_clip.duration)\n",
    "\n",
    "            video_clips.append(video_clip.set_audio(audio_clip))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"🚨 파일 처리 중 오류 발생. 해당 클립을 건너뜁니다: {video_path} ({e})\")\n",
    "            continue\n",
    "\n",
    "    if not video_clips:\n",
    "        print(\"🚨 처리할 유효한 클립이 없어 영상 생성을 중단합니다.\")\n",
    "        return \"\"\n",
    "    \n",
    "    final_video = concatenate_videoclips(video_clips)\n",
    "    \n",
    "    if background_music_path and os.path.exists(background_music_path):\n",
    "        bg_music_original = AudioFileClip(background_music_path).volumex(0.3)\n",
    "        bg_music = afx.audio_loop(bg_music_original, duration=final_video.duration)\n",
    "        # final_video.set_audio(CompositeAudioClip([final_video.audio, bg_music]))\n",
    "        final_video = final_video.set_audio(CompositeAudioClip([final_video.audio, bg_music]))\n",
    "\n",
    "\n",
    "    output_path = f\"{OUTPUT_DIR}/final_video_with_continuity.mp4\"\n",
    "    final_video.write_videofile(\n",
    "        output_path, codec='libx264', audio_codec='aac', threads=4, preset='medium'\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ 최종 영상 생성 완료: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. 전체 파이프라인 실행 함수 (MAIN ORCHESTRATOR)\n",
    "# ==============================================================================\n",
    "\n",
    "def run_full_pipeline_with_continuity(\n",
    "    g_client: genai.Client,\n",
    "    t_client: texttospeech.TextToSpeechClient,\n",
    "    video_description: str,\n",
    "    lang_code: str = \"ko-KR\"\n",
    ") -> str:\n",
    "    \"\"\"연속성을 고려한 전체 멀티모달 영상 생성 파이프라인을 실행합니다.\"\"\"\n",
    "    print(\"\\n🚀 연속성 있는 영상 생성 파이프라인 시작...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    storyline = generate_storyline(g_client, video_description)\n",
    "    \n",
    "    video_paths = []\n",
    "    audio_paths = []\n",
    "    prior_image_path = None\n",
    "    \n",
    "    for idx, scene in enumerate(storyline['scenes']):\n",
    "        print(f\"\\n--- SCENE {idx + 1}/{len(storyline['scenes'])} 처리 시작 ---\")\n",
    "        \n",
    "        image_path = generate_image_with_prior_context(g_client, scene['story'], prior_image_path, idx)\n",
    "        video_path = generate_video_from_image(g_client, image_path, scene['story'], idx)\n",
    "        audio_path = generate_tts_audio(t_client, scene['script'], idx, lang_code)\n",
    "        \n",
    "        video_paths.append(video_path)\n",
    "        audio_paths.append(audio_path)\n",
    "        \n",
    "        prior_image_path = extract_last_frame(video_path)\n",
    "\n",
    "    background_music_path = generate_background_music(storyline['music'], 30)\n",
    "    \n",
    "    final_video_path = combine_video_clips(video_paths, audio_paths, background_music_path)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\n🎉 파이프라인 완료! 총 소요시간: {elapsed_time:.2f}초\")\n",
    "    print(f\"📁 최종 영상: {final_video_path}\")\n",
    "    \n",
    "    return final_video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90816b80-4561-4e55-acbf-4f6cd9658fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e79da914-4a49-4929-a9f9-00d0c71e4e3f",
   "metadata": {},
   "source": [
    "## Test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bba21ae-18da-4fdc-a674-d002799184ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 연속성 있는 영상 생성 파이프라인 시작...\n",
      "🎬 1단계: 스토리라인 생성 중...\n",
      "✅ 스토리라인 생성 완료: 7개 장면\n",
      "\n",
      "--- SCENE 1/7 처리 시작 ---\n",
      "🖼️  2단계: 장면 1 이미지 생성 중...\n",
      "✅ 장면 1 이미지 생성 완료: veo_story_telling/scene_0_image.png\n",
      "📹 3단계: 장면 1 비디오 생성 중...\n",
      "...VEO가 비디오를 생성하는 동안 대기합니다 (약 1~2분 소요)...\n",
      "✅ 장면 1 비디오 저장 완료: 'veo_story_telling/scene_0_video.mp4'\n",
      "🗣️  4단계: 장면 1 TTS 오디오 생성 중...\n",
      "✅ 장면 1 TTS 생성 완료: veo_story_telling/scene_0_audio.mp3\n",
      "✅ 마지막 프레임 추출 완료: veo_story_telling/last_frame_1756426293.png\n",
      "\n",
      "--- SCENE 2/7 처리 시작 ---\n",
      "🖼️  2단계: 장면 2 이미지 생성 중...\n",
      "✅ 장면 2 이미지 생성 완료: veo_story_telling/scene_1_image.png\n",
      "📹 3단계: 장면 2 비디오 생성 중...\n",
      "...VEO가 비디오를 생성하는 동안 대기합니다 (약 1~2분 소요)...\n",
      "✅ 장면 2 비디오 저장 완료: 'veo_story_telling/scene_1_video.mp4'\n",
      "🗣️  4단계: 장면 2 TTS 오디오 생성 중...\n",
      "✅ 장면 2 TTS 생성 완료: veo_story_telling/scene_1_audio.mp3\n",
      "✅ 마지막 프레임 추출 완료: veo_story_telling/last_frame_1756426385.png\n",
      "\n",
      "--- SCENE 3/7 처리 시작 ---\n",
      "🖼️  2단계: 장면 3 이미지 생성 중...\n",
      "✅ 장면 3 이미지 생성 완료: veo_story_telling/scene_2_image.png\n",
      "📹 3단계: 장면 3 비디오 생성 중...\n",
      "...VEO가 비디오를 생성하는 동안 대기합니다 (약 1~2분 소요)...\n",
      "✅ 장면 3 비디오 저장 완료: 'veo_story_telling/scene_2_video.mp4'\n",
      "🗣️  4단계: 장면 3 TTS 오디오 생성 중...\n",
      "✅ 장면 3 TTS 생성 완료: veo_story_telling/scene_2_audio.mp3\n",
      "✅ 마지막 프레임 추출 완료: veo_story_telling/last_frame_1756426485.png\n",
      "\n",
      "--- SCENE 4/7 처리 시작 ---\n",
      "🖼️  2단계: 장면 4 이미지 생성 중...\n",
      "✅ 장면 4 이미지 생성 완료: veo_story_telling/scene_3_image.png\n",
      "📹 3단계: 장면 4 비디오 생성 중...\n",
      "...VEO가 비디오를 생성하는 동안 대기합니다 (약 1~2분 소요)...\n",
      "✅ 장면 4 비디오 저장 완료: 'veo_story_telling/scene_3_video.mp4'\n",
      "🗣️  4단계: 장면 4 TTS 오디오 생성 중...\n",
      "✅ 장면 4 TTS 생성 완료: veo_story_telling/scene_3_audio.mp3\n",
      "✅ 마지막 프레임 추출 완료: veo_story_telling/last_frame_1756426577.png\n",
      "\n",
      "--- SCENE 5/7 처리 시작 ---\n",
      "🖼️  2단계: 장면 5 이미지 생성 중...\n",
      "✅ 장면 5 이미지 생성 완료: veo_story_telling/scene_4_image.png\n",
      "📹 3단계: 장면 5 비디오 생성 중...\n",
      "...VEO가 비디오를 생성하는 동안 대기합니다 (약 1~2분 소요)...\n",
      "✅ 장면 5 비디오 저장 완료: 'veo_story_telling/scene_4_video.mp4'\n",
      "🗣️  4단계: 장면 5 TTS 오디오 생성 중...\n",
      "✅ 장면 5 TTS 생성 완료: veo_story_telling/scene_4_audio.mp3\n",
      "✅ 마지막 프레임 추출 완료: veo_story_telling/last_frame_1756426651.png\n",
      "\n",
      "--- SCENE 6/7 처리 시작 ---\n",
      "🖼️  2단계: 장면 6 이미지 생성 중...\n",
      "✅ 장면 6 이미지 생성 완료: veo_story_telling/scene_5_image.png\n",
      "📹 3단계: 장면 6 비디오 생성 중...\n",
      "...VEO가 비디오를 생성하는 동안 대기합니다 (약 1~2분 소요)...\n",
      "✅ 장면 6 비디오 저장 완료: 'veo_story_telling/scene_5_video.mp4'\n",
      "🗣️  4단계: 장면 6 TTS 오디오 생성 중...\n",
      "✅ 장면 6 TTS 생성 완료: veo_story_telling/scene_5_audio.mp3\n",
      "✅ 마지막 프레임 추출 완료: veo_story_telling/last_frame_1756426741.png\n",
      "\n",
      "--- SCENE 7/7 처리 시작 ---\n",
      "🖼️  2단계: 장면 7 이미지 생성 중...\n",
      "✅ 장면 7 이미지 생성 완료: veo_story_telling/scene_6_image.png\n",
      "📹 3단계: 장면 7 비디오 생성 중...\n",
      "...VEO가 비디오를 생성하는 동안 대기합니다 (약 1~2분 소요)...\n",
      "✅ 장면 7 비디오 저장 완료: 'veo_story_telling/scene_6_video.mp4'\n",
      "🗣️  4단계: 장면 7 TTS 오디오 생성 중...\n",
      "✅ 장면 7 TTS 생성 완료: veo_story_telling/scene_6_audio.mp3\n",
      "✅ 마지막 프레임 추출 완료: veo_story_telling/last_frame_1756426830.png\n",
      "🎵 5단계: 배경음악 생성 중... (설명: 'Orchestral, majestic, epic, inspiring, full of wonder, soaring strings and deep brass.')\n",
      "✅ 배경음악 생성 완료: veo_story_telling/background_music.mp3\n",
      "🖇️  6단계: 최종 영상 병합 시작...\n",
      "Moviepy - Building video veo_story_telling/final_video_with_continuity.mp4.\n",
      "MoviePy - Writing audio in final_video_with_continuityTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video veo_story_telling/final_video_with_continuity.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready veo_story_telling/final_video_with_continuity.mp4\n",
      "✅ 최종 영상 생성 완료: veo_story_telling/final_video_with_continuity.mp4\n",
      "\n",
      "🎉 파이프라인 완료! 총 소요시간: 716.54초\n",
      "📁 최종 영상: veo_story_telling/final_video_with_continuity.mp4\n",
      "\n",
      "✅ 최종 비디오가 성공적으로 생성되었습니다: veo_story_telling/final_video_with_continuity.mp4\n"
     ]
    }
   ],
   "source": [
    "# --- 여기에 만들고 싶은 영상에 대한 설명을 입력하세요 --- # 태양계\n",
    "video_description_prompt = \"\"\"\n",
    "Create a short, epic video journey through our solar system.\n",
    "The video should travel outwards from the Sun, passing by Mercury, Venus, Earth, Mars, and then flying through the asteroid belt towards Jupiter.\n",
    "Make it awe-inspiring and suitable for a documentary opening.\n",
    "Each scene should transition smoothly to the next.\n",
    "\"\"\"\n",
    "\n",
    "# 파이프라인 실행\n",
    "# 전역으로 생성된 클라이언트 객체를 전달합니다.\n",
    "final_video = run_full_pipeline_with_continuity(\n",
    "    g_client=genai_client,\n",
    "    t_client=tts_client,\n",
    "    video_description=video_description_prompt\n",
    ")\n",
    "\n",
    "if final_video:\n",
    "    print(f\"\\n✅ 최종 비디오가 성공적으로 생성되었습니다: {final_video}\")\n",
    "else:\n",
    "    print(\"\\n🚨 최종 비디오 생성에 실패했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b94f62-87e8-4164-a821-48215b3f535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 여기에 만들고 싶은 영상에 대한 설명을 입력하세요 ---\n",
    "# video_description_prompt = \"\"\"\n",
    "# Create a short, dynamic time-lapse video of a skyscraper being built.\n",
    "# The video should show the progression from an empty construction site with foundations being laid, to the steel framework rising floor by floor, then the glass exterior being installed, and finally the completed tower standing tall in the city skyline.\n",
    "# Make it feel powerful and modern.\n",
    "# Each scene should transition smoothly to the next.\n",
    "# \"\"\"\n",
    "\n",
    "# # 파이프라인 실행\n",
    "# # 전역으로 생성된 클라이언트 객체를 전달합니다.\n",
    "# final_video = run_full_pipeline_with_continuity(\n",
    "#     g_client=genai_client,\n",
    "#     t_client=tts_client,\n",
    "#     video_description=video_description_prompt\n",
    "# )\n",
    "\n",
    "# if final_video:\n",
    "#     print(f\"\\n✅ 최종 비디오가 성공적으로 생성되었습니다: {final_video}\")\n",
    "# else:\n",
    "#     print(\"\\n🚨 최종 비디오 생성에 실패했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece96448-9a2b-4cbd-a61b-d11289719502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 여기에 만들고 싶은 영상에 대한 설명을 입력하세요 --- ## 책만들기\n",
    "# video_description_prompt = \"\"\"\n",
    "# Create a short educational video on how a book is made.\n",
    "# The video should flow from a large roll of paper being fed into a printing press, to the printed pages being cut and stacked, then the cover being attached, and finally the finished books being boxed for shipping.\n",
    "# Make it informative and fascinating.\n",
    "# Each scene should transition smoothly to the next.\n",
    "# \"\"\"\n",
    "\n",
    "# # 파이프라인 실행\n",
    "# # 전역으로 생성된 클라이언트 객체를 전달합니다.\n",
    "# final_video = run_full_pipeline_with_continuity(\n",
    "#     g_client=genai_client,\n",
    "#     t_client=tts_client,\n",
    "#     video_description=video_description_prompt\n",
    "# )\n",
    "\n",
    "# if final_video:\n",
    "#     print(f\"\\n✅ 최종 비디오가 성공적으로 생성되었습니다: {final_video}\")\n",
    "# else:\n",
    "#     print(\"\\n🚨 최종 비디오 생성에 실패했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f0a04-7296-4af5-ba2f-7b968f340a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7186a16-4296-4fa7-a276-51a302f857f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fbbe9-6b51-4918-a3a2-a16585589313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-myenv-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
