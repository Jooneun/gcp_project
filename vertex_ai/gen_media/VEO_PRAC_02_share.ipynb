{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95cba2ec-d6e6-4c79-935c-a26a902fd66a",
   "metadata": {},
   "source": [
    "## CE Enablement Session - Practice 02\n",
    "- ## Workflow: Story telling: IMG --> VEO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0034fcc8-e2ed-4ad2-96bf-b1e8f240c788",
   "metadata": {},
   "source": [
    "### 1. (ê¶Œì¥) ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”\n",
    "python -m venv venv\n",
    "#### macOS/Linux:\n",
    "source venv/bin/activate\n",
    "\n",
    "#### 2. í•„ìˆ˜ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "pip install google-cloud-aiplatform google-cloud-texttospeech moviepy Pillow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "765d6455-dbb3-4cca-b05b-60668b307dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...\n",
      "âœ… API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. íŒ¨í‚¤ì§€ IMPORT\n",
    "# ==============================================================================\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import base64\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Google Cloud & GenAI\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import requests\n",
    "from google.cloud import texttospeech\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "# Media Processing (MoviePy & PIL)\n",
    "from moviepy.editor import (\n",
    "    VideoFileClip, AudioFileClip, CompositeVideoClip,\n",
    "    concatenate_videoclips, ImageClip, CompositeAudioClip\n",
    ")\n",
    "import moviepy.video.fx.all as vfx\n",
    "import moviepy.audio.fx.all as afx\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. ì „ì—­ ì„¤ì • (CONFIGURATION)\n",
    "# ==============================================================================\n",
    "# --- Google Cloud & Vertex AI ì„¤ì • ---\n",
    "PROJECT_ID = \"jc-gcp-project\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "# --- ëª¨ë¸ ì´ë¦„ ì„¤ì • ---\n",
    "STORY_MODEL = \"gemini-2.5-flash\"\n",
    "IMAGEN_MODEL = \"imagen-4.0-generate-preview-06-06\"\n",
    "VEO_MODEL = \"veo-3.0-generate-preview\"\n",
    "LYRIA_MODEL_ENDPOINT = (\n",
    "    f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}\"\n",
    "    f\"/locations/us-central1/publishers/google/models/lyria-002:predict\"\n",
    ")\n",
    "\n",
    "# --- TTS ëª©ì†Œë¦¬ ì„¤ì • ---\n",
    "TTS_VOICE_NAME = \"ko-KR-Chirp3-HD-Callirrhoe\" # https://cloud.google.com/text-to-speech/docs/list-voices-and-types\n",
    "\n",
    "\n",
    "# --- ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì • ---\n",
    "OUTPUT_DIR = \"veo_story_telling\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (CLIENT INITIALIZATION)\n",
    "# ==============================================================================\n",
    "# ê° API í´ë¼ì´ì–¸íŠ¸ë¥¼ ì²˜ìŒì— í•œ ë²ˆë§Œ ìƒì„±í•˜ì—¬ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "print(\"API í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...\")\n",
    "genai_client = genai.Client(vertexai=True, project=PROJECT_ID, location=\"global\")\n",
    "tts_client = texttospeech.TextToSpeechClient()\n",
    "print(\"âœ… API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. ê¸°ëŠ¥ë³„ í•¨ìˆ˜ ì •ì˜ (FUNCTIONS)\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_storyline(\n",
    "    client: genai.Client,\n",
    "    video_description: str,\n",
    "    min_scenes: int = 1\n",
    ") -> Dict:\n",
    "    \"\"\"1ë‹¨ê³„: Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ ìŠ¤í† ë¦¬ë¼ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"ğŸ¬ 1ë‹¨ê³„: ìŠ¤í† ë¦¬ë¼ì¸ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    # í•¨ìˆ˜ ë‚´ì—ì„œ í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ëŒ€ì‹ , íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ë°›ì•„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    model = STORY_MODEL\n",
    "    \n",
    "    storyline_schema = {\n",
    "    \"type\": \"OBJECT\",\n",
    "    \"properties\": {\n",
    "        \"description\": {\"type\": \"STRING\", \"description\": \"Summary of the video\"},\n",
    "        \"music\": {\"type\": \"STRING\", \"description\": \"Description of music style (15 words or less)\"},\n",
    "        \"scenes\": {\n",
    "            \"type\": \"ARRAY\",\n",
    "            \"description\": f\"An array of at least {min_scenes} scenes for the video.\",\n",
    "            \"items\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\n",
    "                    \"story\": {\"type\": \"STRING\", \"description\": \"Storyline for this specific scene\"},\n",
    "                    \"script\": {\"type\": \"STRING\", \"description\": \"Voiceover script for this scene\"}\n",
    "                },\n",
    "                \"required\": [\"story\", \"script\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"description\", \"music\", \"scenes\"]}\n",
    "\n",
    "    prompt = f\"\"\"You are a professional video producer.\n",
    "                Create a storyline for video based on the content below.\n",
    "\n",
    "                [VIDEO_DESCRIPTION]\n",
    "                {video_description}\n",
    "\n",
    "                Please create a storyline at least {min_scenes} scenes.\n",
    "                Each scene should contain one key piece of information and flow naturally to the next scene.\n",
    "\n",
    "                Use this JSON schema for output:\n",
    "                {{\n",
    "                    \"description\": \"Summary of video\",\n",
    "                    \"music\": \"Description of music style (15 words or less)\",\n",
    "                    \"scenes\": [\n",
    "                        {{\"story\": \"Storyline for the first scene\", \"script\": \"Script for voice\"}},\n",
    "                        {{\"story\": \"Storyline for the second scene\", \"script\": \"Script for voice\"}},\n",
    "                        ...\n",
    "                    ]\n",
    "                }}\"\"\"\n",
    "    \n",
    "    contents = [types.Content(role=\"user\", parts=[types.Part.from_text(text=prompt)])]\n",
    "\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=0.95,\n",
    "        seed=0,\n",
    "        max_output_tokens=8192,\n",
    "        response_mime_type = \"application/json\",\n",
    "        response_schema=storyline_schema,\n",
    "        system_instruction=[types.Part.from_text(text=\"\"\"Script must be in KOREAN. Music must be in English\"\"\")],\n",
    "    )\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config\n",
    "    )\n",
    "    \n",
    "    clean_text = response.text.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "    storyline = json.loads(clean_text)\n",
    "    print('###### storyline í™•ì¸ í•˜ê¸° ######')\n",
    "    print(storyline)\n",
    "    print(f\"âœ… ìŠ¤í† ë¦¬ë¼ì¸ ìƒì„± ì™„ë£Œ: {len(storyline['scenes'])}ê°œ ì¥ë©´\")\n",
    "    return storyline\n",
    "\n",
    "def generate_image_with_prior_context(\n",
    "    client: genai.Client,\n",
    "    scene_story: str,\n",
    "    prior_image_path: Optional[str] = None,\n",
    "    scene_idx: int = 0\n",
    ") -> str:\n",
    "    \"\"\"2ë‹¨ê³„: ì´ì „ ì´ë¯¸ì§€ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì¥ë©´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(f\"ğŸ–¼ï¸  2ë‹¨ê³„: ì¥ë©´ {scene_idx + 1} ì´ë¯¸ì§€ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    if prior_image_path and os.path.exists(prior_image_path):\n",
    "        prompt_text = f\"\"\"You are producing a video, and you want to create an image for the next scene.\n",
    "                        The previous scene ended with the image below, and now you need to create an image for this scene:\n",
    "                        [SCENE_STORY] {scene_story}\n",
    "                        Please create a prompt for image generation that references the previous image for visual continuity and creates a natural transition.\n",
    "                        Use this JSON schema for output: {{\"prompt\": \"prompt for image generation with continuity\"}}\"\"\"\n",
    "        \n",
    "        schema = {\"type\": \"OBJECT\", \"properties\": {\"prompt\": {\"type\": \"STRING\"}}, \"required\": [\"prompt\"]}\n",
    "        \n",
    "        with open(prior_image_path, \"rb\") as f:\n",
    "            image_bytes = f.read()\n",
    "        image_part = types.Part.from_bytes(data=image_bytes, mime_type=\"image/png\")\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=STORY_MODEL,\n",
    "            contents=[prompt_text, image_part],\n",
    "            config=types.GenerateContentConfig(response_mime_type=\"application/json\", response_schema=schema)\n",
    "        )\n",
    "        image_prompt = json.loads(response.text)[\"prompt\"]\n",
    "    else:\n",
    "        image_prompt = f\"Create a high-quality cinematic image for this scene: {scene_story}. The image should be suitable for video generation.\"\n",
    "\n",
    "    response_imagen = client.models.generate_images(\n",
    "        model=IMAGEN_MODEL,\n",
    "        prompt=image_prompt,\n",
    "        config=types.GenerateImagesConfig(aspect_ratio=\"16:9\", number_of_images=1),\n",
    "    )\n",
    "    \n",
    "    image_path = f\"{OUTPUT_DIR}/scene_{scene_idx}_image.png\"\n",
    "    generated_image_bytes = response_imagen.generated_images[0].image.image_bytes\n",
    "    with open(image_path, \"wb\") as f:\n",
    "        f.write(generated_image_bytes)\n",
    "\n",
    "    print(f\"âœ… ì¥ë©´ {scene_idx + 1} ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {image_path}\")\n",
    "    return image_path\n",
    "\n",
    "def generate_video_from_image(\n",
    "    client: genai.Client,\n",
    "    image_path: str,\n",
    "    scene_story: str,\n",
    "    scene_idx: int\n",
    ") -> str:\n",
    "    \"\"\"3ë‹¨ê³„: ìƒì„±ëœ ì´ë¯¸ì§€ë¡œë¶€í„° VEO ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. [Image í´ë˜ìŠ¤ ë³µì›ë¨]\"\"\"\n",
    "    print(f\"ğŸ“¹ 3ë‹¨ê³„: ì¥ë©´ {scene_idx + 1} ë¹„ë””ì˜¤ ìƒì„± ì¤‘...\")\n",
    "\n",
    "    # --- [ìˆ˜ì •] VEO APIê°€ ìš”êµ¬í•˜ëŠ” íŠ¹ì • ê°ì²´ êµ¬ì¡°ë¥¼ ìœ„í•´ ë‚´ë¶€ Image í´ë˜ìŠ¤ë¥¼ ë³µì›í•©ë‹ˆë‹¤ ---\n",
    "    # ì´ í´ë˜ìŠ¤ëŠ” generate_videos í•¨ìˆ˜ì˜ image íŒŒë¼ë¯¸í„°ì™€ í˜¸í™˜ë©ë‹ˆë‹¤.\n",
    "    import mimetypes\n",
    "\n",
    "    class ImageForVEO:\n",
    "        def __init__(self, gcs_uri=None, image_bytes=None, mime_type=None):\n",
    "            self.gcs_uri = gcs_uri\n",
    "            self.image_bytes = image_bytes\n",
    "            self.mime_type = mime_type\n",
    "\n",
    "    try:\n",
    "        mime_type, _ = mimetypes.guess_type(image_path)\n",
    "        if not mime_type:\n",
    "            raise ValueError(f\"Could not determine MIME type for {image_path}\")\n",
    "\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            image_byte_data = image_file.read()\n",
    "\n",
    "        # APIê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ì˜ ì´ë¯¸ì§€ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "        image_instance = ImageForVEO(\n",
    "            gcs_uri=None,\n",
    "            image_bytes=image_byte_data,\n",
    "            mime_type=mime_type\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"Generate a video from this image that shows: {scene_story}.\n",
    "                    The video should be smooth and cinematic, lasting 3-5 seconds.\"\"\"\n",
    "        \n",
    "        operation = client.models.generate_videos(\n",
    "            model=VEO_MODEL,\n",
    "            prompt=prompt,\n",
    "            image=image_instance, # ë³µì›ëœ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©\n",
    "        )\n",
    "\n",
    "        print(\"...VEOê°€ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” ë™ì•ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤ (ì•½ 1~2ë¶„ ì†Œìš”)...\")\n",
    "        while not operation.done:\n",
    "            time.sleep(10)\n",
    "            operation = client.operations.get(operation)\n",
    "\n",
    "        if operation.error:\n",
    "            error_message = getattr(operation.error, 'message', str(operation.error))\n",
    "            raise Exception(f\"ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: {error_message}\")\n",
    "\n",
    "        video_bytes = operation.response.generated_videos[0].video.video_bytes\n",
    "        video_path = f\"{OUTPUT_DIR}/scene_{scene_idx}_video.mp4\"\n",
    "        with open(video_path, 'wb') as f:\n",
    "            f.write(video_bytes)\n",
    "        \n",
    "        print(f\"âœ… ì¥ë©´ {scene_idx + 1} ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: '{video_path}'\")\n",
    "        return video_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì¥ë©´ {scene_idx + 1} ë¹„ë””ì˜¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return \"\" # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ê²½ë¡œ ë°˜í™˜\n",
    "\n",
    "\n",
    "def generate_tts_audio(\n",
    "    client: texttospeech.TextToSpeechClient,\n",
    "    script: str,\n",
    "    scene_idx: int,\n",
    "    lang_code: str = \"ko-KR\"\n",
    ") -> str:\n",
    "    \"\"\"4ë‹¨ê³„: TTS ì˜¤ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(f\"ğŸ—£ï¸  4ë‹¨ê³„: ì¥ë©´ {scene_idx + 1} TTS ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    synthesis_input = texttospeech.SynthesisInput(text=script)\n",
    "    voice = texttospeech.VoiceSelectionParams(language_code=lang_code, name=TTS_VOICE_NAME)\n",
    "    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "    \n",
    "    response = client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)\n",
    "    \n",
    "    audio_path = f\"{OUTPUT_DIR}/scene_{scene_idx}_audio.mp3\"\n",
    "    with open(audio_path, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "    \n",
    "    print(f\"âœ… ì¥ë©´ {scene_idx + 1} TTS ìƒì„± ì™„ë£Œ: {audio_path}\")\n",
    "    return audio_path\n",
    "\n",
    "def generate_background_music(music_description: str, duration: int) -> str:\n",
    "    \"\"\"5ë‹¨ê³„: Lyria APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ê²½ ìŒì•…ì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(f\"ğŸµ 5ë‹¨ê³„: ë°°ê²½ìŒì•… ìƒì„± ì¤‘... (ì„¤ëª…: '{music_description}')\")\n",
    "    \n",
    "    def send_request_with_retry(data=None, max_retries=3):\n",
    "        creds, _ = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                creds.refresh(auth_req)\n",
    "                headers[\"Authorization\"] = f\"Bearer {creds.token}\"\n",
    "                response = requests.post(LYRIA_MODEL_ENDPOINT, headers=headers, json=data)\n",
    "                response.raise_for_status()\n",
    "                return response.json()\n",
    "            except HTTPError as e:\n",
    "                print(f\"âŒ HTTP ì—ëŸ¬ (ì‹œë„ {attempt + 1}/{max_retries}): {e}\")\n",
    "                if e.response:\n",
    "                    print(f\"  - ì„œë²„ ì‘ë‹µ: {e.response.text}\")\n",
    "                if attempt + 1 < max_retries:\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    raise\n",
    "        raise Exception(\"API ìš”ì²­ ìµœì¢… ì‹¤íŒ¨\")\n",
    "\n",
    "    request_data = {\n",
    "      \"instances\": [{\"prompt\": music_description, \"negative_prompt\": \"dark\"}],\n",
    "      \"parameters\": {\"duration_seconds\": int(duration), \"sample_count\": 1}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        predictions = send_request_with_retry(request_data)[\"predictions\"]\n",
    "        b64_audio_data = predictions[0]['bytesBase64Encoded']\n",
    "        music_path = f\"{OUTPUT_DIR}/background_music.mp3\"\n",
    "        with open(music_path, \"wb\") as f:\n",
    "            f.write(base64.b64decode(b64_audio_data))\n",
    "        print(f\"âœ… ë°°ê²½ìŒì•… ìƒì„± ì™„ë£Œ: {music_path}\")\n",
    "        return music_path\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ ë°°ê²½ìŒì•… ìƒì„± ì‹¤íŒ¨: {e}. ë°°ê²½ìŒì•… ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_last_frame(video_path: str) -> Optional[str]:\n",
    "    \"\"\"ì˜ìƒì—ì„œ ë§ˆì§€ë§‰ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ ë‹¤ìŒ ì¥ë©´ ìƒì„±ì— ì‚¬ìš©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"âš ï¸ ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ì¶œ ê±´ë„ˆë›°ê¸°: '{video_path}' íŒŒì¼ ì—†ìŒ.\")\n",
    "        return None\n",
    "    try:\n",
    "        with VideoFileClip(video_path) as clip:\n",
    "            last_frame_time = clip.duration - (1 / clip.fps)\n",
    "            last_frame = clip.get_frame(last_frame_time)\n",
    "            \n",
    "            last_frame_image = Image.fromarray(last_frame)\n",
    "            last_frame_path = f\"{OUTPUT_DIR}/last_frame_{int(time.time())}.png\"\n",
    "            last_frame_image.save(last_frame_path)\n",
    "            \n",
    "            print(f\"âœ… ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ: {last_frame_path}\")\n",
    "            return last_frame_path\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def combine_video_clips(\n",
    "    video_paths: list[str],\n",
    "    audio_paths: list[str],\n",
    "    background_music_path: str\n",
    ") -> str:\n",
    "    \"\"\"6ë‹¨ê³„: ëª¨ë“  ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤, ë°°ê²½ìŒì•…ì„ ë³‘í•©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"ğŸ–‡ï¸  6ë‹¨ê³„: ìµœì¢… ì˜ìƒ ë³‘í•© ì‹œì‘...\")\n",
    "\n",
    "    def sanitize_video_clip(video):\n",
    "        \"\"\"ë¹„ë””ì˜¤ì˜ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸¸ì´ë¥¼ ì°¾ì•„ í´ë¦½ì„ ì•ˆì •í™”ì‹œí‚µë‹ˆë‹¤.\"\"\"\n",
    "        try:\n",
    "            if not all([hasattr(video, 'fps'), hasattr(video, 'duration'), video.fps, video.duration]):\n",
    "                return video\n",
    "            frame_time = 1 / video.fps\n",
    "            real_duration = video.duration\n",
    "            for frame_number in reversed(range(int(video.fps * video.duration))):\n",
    "                try:\n",
    "                    current_time = frame_number * frame_time\n",
    "                    video.get_frame(current_time)\n",
    "                    real_duration = current_time + frame_time\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "            if abs(real_duration - video.duration) > frame_time:\n",
    "                print(f\"âœ‚ï¸ í´ë¦½ ì•ˆì •í™”: {video.duration:.2f}s -> {real_duration:.2f}s\")\n",
    "                return video.subclip(0, real_duration)\n",
    "            return video\n",
    "        except Exception as e:\n",
    "            print(f\"ğŸš¨ í´ë¦½ ì•ˆì •í™” ì¤‘ ì˜¤ë¥˜: {e}. ì›ë³¸ í´ë¦½ ì‚¬ìš©.\")\n",
    "            return video\n",
    "\n",
    "    video_clips = []\n",
    "    for video_path, audio_path in zip(video_paths, audio_paths):\n",
    "        try:\n",
    "            if not os.path.exists(video_path) or os.path.getsize(video_path) < 1024:\n",
    "                print(f\"âš ï¸ íŒŒì¼ì´ ì—†ê±°ë‚˜ ì†ìƒë˜ì–´ ê±´ë„ˆëœë‹ˆë‹¤: {video_path}\")\n",
    "                continue\n",
    "\n",
    "            video_clip = VideoFileClip(video_path)\n",
    "            video_clip = sanitize_video_clip(video_clip)\n",
    "            audio_clip = AudioFileClip(audio_path)\n",
    "            \n",
    "            if video_clip.duration < audio_clip.duration:\n",
    "                speed_factor = video_clip.duration / audio_clip.duration\n",
    "                video_clip = vfx.speedx(video_clip, factor=speed_factor).set_duration(audio_clip.duration)\n",
    "            else:\n",
    "                video_clip = video_clip.subclip(0, audio_clip.duration)\n",
    "\n",
    "            video_clips.append(video_clip.set_audio(audio_clip))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ğŸš¨ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. í•´ë‹¹ í´ë¦½ì„ ê±´ë„ˆëœë‹ˆë‹¤: {video_path} ({e})\")\n",
    "            continue\n",
    "\n",
    "    if not video_clips:\n",
    "        print(\"ğŸš¨ ì²˜ë¦¬í•  ìœ íš¨í•œ í´ë¦½ì´ ì—†ì–´ ì˜ìƒ ìƒì„±ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")\n",
    "        return \"\"\n",
    "    \n",
    "    final_video = concatenate_videoclips(video_clips)\n",
    "    \n",
    "    if background_music_path and os.path.exists(background_music_path):\n",
    "        bg_music_original = AudioFileClip(background_music_path).volumex(0.3)\n",
    "        bg_music = afx.audio_loop(bg_music_original, duration=final_video.duration)\n",
    "        # final_video.set_audio(CompositeAudioClip([final_video.audio, bg_music]))\n",
    "        final_video = final_video.set_audio(CompositeAudioClip([final_video.audio, bg_music]))\n",
    "\n",
    "\n",
    "    output_path = f\"{OUTPUT_DIR}/final_video_with_continuity.mp4\"\n",
    "    final_video.write_videofile(\n",
    "        output_path, codec='libx264', audio_codec='aac', threads=4, preset='medium'\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… ìµœì¢… ì˜ìƒ ìƒì„± ì™„ë£Œ: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ (MAIN ORCHESTRATOR)\n",
    "# ==============================================================================\n",
    "\n",
    "def run_full_pipeline_with_continuity(\n",
    "    g_client: genai.Client,\n",
    "    t_client: texttospeech.TextToSpeechClient,\n",
    "    video_description: str,\n",
    "    lang_code: str = \"ko-KR\"\n",
    ") -> str:\n",
    "    \"\"\"ì—°ì†ì„±ì„ ê³ ë ¤í•œ ì „ì²´ ë©€í‹°ëª¨ë‹¬ ì˜ìƒ ìƒì„± íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"\\nğŸš€ ì—°ì†ì„± ìˆëŠ” ì˜ìƒ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹œì‘...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    storyline = generate_storyline(g_client, video_description)\n",
    "    \n",
    "    video_paths = []\n",
    "    audio_paths = []\n",
    "    prior_image_path = None\n",
    "    \n",
    "    for idx, scene in enumerate(storyline['scenes']):\n",
    "        print(f\"\\n--- SCENE {idx + 1}/{len(storyline['scenes'])} ì²˜ë¦¬ ì‹œì‘ ---\")\n",
    "        \n",
    "        image_path = generate_image_with_prior_context(g_client, scene['story'], prior_image_path, idx)\n",
    "        video_path = generate_video_from_image(g_client, image_path, scene['story'], idx)\n",
    "        audio_path = generate_tts_audio(t_client, scene['script'], idx, lang_code)\n",
    "        \n",
    "        video_paths.append(video_path)\n",
    "        audio_paths.append(audio_path)\n",
    "        \n",
    "        prior_image_path = extract_last_frame(video_path)\n",
    "\n",
    "    background_music_path = generate_background_music(storyline['music'], 30)\n",
    "    \n",
    "    final_video_path = combine_video_clips(video_paths, audio_paths, background_music_path)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ\")\n",
    "    print(f\"ğŸ“ ìµœì¢… ì˜ìƒ: {final_video_path}\")\n",
    "    \n",
    "    return final_video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90816b80-4561-4e55-acbf-4f6cd9658fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e79da914-4a49-4929-a9f9-00d0c71e4e3f",
   "metadata": {},
   "source": [
    "## Test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bba21ae-18da-4fdc-a674-d002799184ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ ì—°ì†ì„± ìˆëŠ” ì˜ìƒ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹œì‘...\n",
      "ğŸ¬ 1ë‹¨ê³„: ìŠ¤í† ë¦¬ë¼ì¸ ìƒì„± ì¤‘...\n",
      "âœ… ìŠ¤í† ë¦¬ë¼ì¸ ìƒì„± ì™„ë£Œ: 7ê°œ ì¥ë©´\n",
      "\n",
      "--- SCENE 1/7 ì²˜ë¦¬ ì‹œì‘ ---\n",
      "ğŸ–¼ï¸  2ë‹¨ê³„: ì¥ë©´ 1 ì´ë¯¸ì§€ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 1 ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: veo_story_telling/scene_0_image.png\n",
      "ğŸ“¹ 3ë‹¨ê³„: ì¥ë©´ 1 ë¹„ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "...VEOê°€ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” ë™ì•ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤ (ì•½ 1~2ë¶„ ì†Œìš”)...\n",
      "âœ… ì¥ë©´ 1 ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: 'veo_story_telling/scene_0_video.mp4'\n",
      "ğŸ—£ï¸  4ë‹¨ê³„: ì¥ë©´ 1 TTS ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 1 TTS ìƒì„± ì™„ë£Œ: veo_story_telling/scene_0_audio.mp3\n",
      "âœ… ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ: veo_story_telling/last_frame_1756426293.png\n",
      "\n",
      "--- SCENE 2/7 ì²˜ë¦¬ ì‹œì‘ ---\n",
      "ğŸ–¼ï¸  2ë‹¨ê³„: ì¥ë©´ 2 ì´ë¯¸ì§€ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 2 ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: veo_story_telling/scene_1_image.png\n",
      "ğŸ“¹ 3ë‹¨ê³„: ì¥ë©´ 2 ë¹„ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "...VEOê°€ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” ë™ì•ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤ (ì•½ 1~2ë¶„ ì†Œìš”)...\n",
      "âœ… ì¥ë©´ 2 ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: 'veo_story_telling/scene_1_video.mp4'\n",
      "ğŸ—£ï¸  4ë‹¨ê³„: ì¥ë©´ 2 TTS ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 2 TTS ìƒì„± ì™„ë£Œ: veo_story_telling/scene_1_audio.mp3\n",
      "âœ… ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ: veo_story_telling/last_frame_1756426385.png\n",
      "\n",
      "--- SCENE 3/7 ì²˜ë¦¬ ì‹œì‘ ---\n",
      "ğŸ–¼ï¸  2ë‹¨ê³„: ì¥ë©´ 3 ì´ë¯¸ì§€ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 3 ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: veo_story_telling/scene_2_image.png\n",
      "ğŸ“¹ 3ë‹¨ê³„: ì¥ë©´ 3 ë¹„ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "...VEOê°€ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” ë™ì•ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤ (ì•½ 1~2ë¶„ ì†Œìš”)...\n",
      "âœ… ì¥ë©´ 3 ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: 'veo_story_telling/scene_2_video.mp4'\n",
      "ğŸ—£ï¸  4ë‹¨ê³„: ì¥ë©´ 3 TTS ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 3 TTS ìƒì„± ì™„ë£Œ: veo_story_telling/scene_2_audio.mp3\n",
      "âœ… ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ: veo_story_telling/last_frame_1756426485.png\n",
      "\n",
      "--- SCENE 4/7 ì²˜ë¦¬ ì‹œì‘ ---\n",
      "ğŸ–¼ï¸  2ë‹¨ê³„: ì¥ë©´ 4 ì´ë¯¸ì§€ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 4 ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: veo_story_telling/scene_3_image.png\n",
      "ğŸ“¹ 3ë‹¨ê³„: ì¥ë©´ 4 ë¹„ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "...VEOê°€ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” ë™ì•ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤ (ì•½ 1~2ë¶„ ì†Œìš”)...\n",
      "âœ… ì¥ë©´ 4 ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: 'veo_story_telling/scene_3_video.mp4'\n",
      "ğŸ—£ï¸  4ë‹¨ê³„: ì¥ë©´ 4 TTS ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 4 TTS ìƒì„± ì™„ë£Œ: veo_story_telling/scene_3_audio.mp3\n",
      "âœ… ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ: veo_story_telling/last_frame_1756426577.png\n",
      "\n",
      "--- SCENE 5/7 ì²˜ë¦¬ ì‹œì‘ ---\n",
      "ğŸ–¼ï¸  2ë‹¨ê³„: ì¥ë©´ 5 ì´ë¯¸ì§€ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 5 ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: veo_story_telling/scene_4_image.png\n",
      "ğŸ“¹ 3ë‹¨ê³„: ì¥ë©´ 5 ë¹„ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "...VEOê°€ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” ë™ì•ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤ (ì•½ 1~2ë¶„ ì†Œìš”)...\n",
      "âœ… ì¥ë©´ 5 ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: 'veo_story_telling/scene_4_video.mp4'\n",
      "ğŸ—£ï¸  4ë‹¨ê³„: ì¥ë©´ 5 TTS ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 5 TTS ìƒì„± ì™„ë£Œ: veo_story_telling/scene_4_audio.mp3\n",
      "âœ… ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ: veo_story_telling/last_frame_1756426651.png\n",
      "\n",
      "--- SCENE 6/7 ì²˜ë¦¬ ì‹œì‘ ---\n",
      "ğŸ–¼ï¸  2ë‹¨ê³„: ì¥ë©´ 6 ì´ë¯¸ì§€ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 6 ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: veo_story_telling/scene_5_image.png\n",
      "ğŸ“¹ 3ë‹¨ê³„: ì¥ë©´ 6 ë¹„ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "...VEOê°€ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” ë™ì•ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤ (ì•½ 1~2ë¶„ ì†Œìš”)...\n",
      "âœ… ì¥ë©´ 6 ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: 'veo_story_telling/scene_5_video.mp4'\n",
      "ğŸ—£ï¸  4ë‹¨ê³„: ì¥ë©´ 6 TTS ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 6 TTS ìƒì„± ì™„ë£Œ: veo_story_telling/scene_5_audio.mp3\n",
      "âœ… ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ: veo_story_telling/last_frame_1756426741.png\n",
      "\n",
      "--- SCENE 7/7 ì²˜ë¦¬ ì‹œì‘ ---\n",
      "ğŸ–¼ï¸  2ë‹¨ê³„: ì¥ë©´ 7 ì´ë¯¸ì§€ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 7 ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: veo_story_telling/scene_6_image.png\n",
      "ğŸ“¹ 3ë‹¨ê³„: ì¥ë©´ 7 ë¹„ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "...VEOê°€ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” ë™ì•ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤ (ì•½ 1~2ë¶„ ì†Œìš”)...\n",
      "âœ… ì¥ë©´ 7 ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: 'veo_story_telling/scene_6_video.mp4'\n",
      "ğŸ—£ï¸  4ë‹¨ê³„: ì¥ë©´ 7 TTS ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...\n",
      "âœ… ì¥ë©´ 7 TTS ìƒì„± ì™„ë£Œ: veo_story_telling/scene_6_audio.mp3\n",
      "âœ… ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ: veo_story_telling/last_frame_1756426830.png\n",
      "ğŸµ 5ë‹¨ê³„: ë°°ê²½ìŒì•… ìƒì„± ì¤‘... (ì„¤ëª…: 'Orchestral, majestic, epic, inspiring, full of wonder, soaring strings and deep brass.')\n",
      "âœ… ë°°ê²½ìŒì•… ìƒì„± ì™„ë£Œ: veo_story_telling/background_music.mp3\n",
      "ğŸ–‡ï¸  6ë‹¨ê³„: ìµœì¢… ì˜ìƒ ë³‘í•© ì‹œì‘...\n",
      "Moviepy - Building video veo_story_telling/final_video_with_continuity.mp4.\n",
      "MoviePy - Writing audio in final_video_with_continuityTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video veo_story_telling/final_video_with_continuity.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready veo_story_telling/final_video_with_continuity.mp4\n",
      "âœ… ìµœì¢… ì˜ìƒ ìƒì„± ì™„ë£Œ: veo_story_telling/final_video_with_continuity.mp4\n",
      "\n",
      "ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: 716.54ì´ˆ\n",
      "ğŸ“ ìµœì¢… ì˜ìƒ: veo_story_telling/final_video_with_continuity.mp4\n",
      "\n",
      "âœ… ìµœì¢… ë¹„ë””ì˜¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: veo_story_telling/final_video_with_continuity.mp4\n"
     ]
    }
   ],
   "source": [
    "# --- ì—¬ê¸°ì— ë§Œë“¤ê³  ì‹¶ì€ ì˜ìƒì— ëŒ€í•œ ì„¤ëª…ì„ ì…ë ¥í•˜ì„¸ìš” --- # íƒœì–‘ê³„\n",
    "video_description_prompt = \"\"\"\n",
    "Create a short, epic video journey through our solar system.\n",
    "The video should travel outwards from the Sun, passing by Mercury, Venus, Earth, Mars, and then flying through the asteroid belt towards Jupiter.\n",
    "Make it awe-inspiring and suitable for a documentary opening.\n",
    "Each scene should transition smoothly to the next.\n",
    "\"\"\"\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "# ì „ì—­ìœ¼ë¡œ ìƒì„±ëœ í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "final_video = run_full_pipeline_with_continuity(\n",
    "    g_client=genai_client,\n",
    "    t_client=tts_client,\n",
    "    video_description=video_description_prompt\n",
    ")\n",
    "\n",
    "if final_video:\n",
    "    print(f\"\\nâœ… ìµœì¢… ë¹„ë””ì˜¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {final_video}\")\n",
    "else:\n",
    "    print(\"\\nğŸš¨ ìµœì¢… ë¹„ë””ì˜¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b94f62-87e8-4164-a821-48215b3f535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- ì—¬ê¸°ì— ë§Œë“¤ê³  ì‹¶ì€ ì˜ìƒì— ëŒ€í•œ ì„¤ëª…ì„ ì…ë ¥í•˜ì„¸ìš” ---\n",
    "# video_description_prompt = \"\"\"\n",
    "# Create a short, dynamic time-lapse video of a skyscraper being built.\n",
    "# The video should show the progression from an empty construction site with foundations being laid, to the steel framework rising floor by floor, then the glass exterior being installed, and finally the completed tower standing tall in the city skyline.\n",
    "# Make it feel powerful and modern.\n",
    "# Each scene should transition smoothly to the next.\n",
    "# \"\"\"\n",
    "\n",
    "# # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "# # ì „ì—­ìœ¼ë¡œ ìƒì„±ëœ í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "# final_video = run_full_pipeline_with_continuity(\n",
    "#     g_client=genai_client,\n",
    "#     t_client=tts_client,\n",
    "#     video_description=video_description_prompt\n",
    "# )\n",
    "\n",
    "# if final_video:\n",
    "#     print(f\"\\nâœ… ìµœì¢… ë¹„ë””ì˜¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {final_video}\")\n",
    "# else:\n",
    "#     print(\"\\nğŸš¨ ìµœì¢… ë¹„ë””ì˜¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece96448-9a2b-4cbd-a61b-d11289719502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- ì—¬ê¸°ì— ë§Œë“¤ê³  ì‹¶ì€ ì˜ìƒì— ëŒ€í•œ ì„¤ëª…ì„ ì…ë ¥í•˜ì„¸ìš” --- ## ì±…ë§Œë“¤ê¸°\n",
    "# video_description_prompt = \"\"\"\n",
    "# Create a short educational video on how a book is made.\n",
    "# The video should flow from a large roll of paper being fed into a printing press, to the printed pages being cut and stacked, then the cover being attached, and finally the finished books being boxed for shipping.\n",
    "# Make it informative and fascinating.\n",
    "# Each scene should transition smoothly to the next.\n",
    "# \"\"\"\n",
    "\n",
    "# # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "# # ì „ì—­ìœ¼ë¡œ ìƒì„±ëœ í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "# final_video = run_full_pipeline_with_continuity(\n",
    "#     g_client=genai_client,\n",
    "#     t_client=tts_client,\n",
    "#     video_description=video_description_prompt\n",
    "# )\n",
    "\n",
    "# if final_video:\n",
    "#     print(f\"\\nâœ… ìµœì¢… ë¹„ë””ì˜¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {final_video}\")\n",
    "# else:\n",
    "#     print(\"\\nğŸš¨ ìµœì¢… ë¹„ë””ì˜¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f0a04-7296-4af5-ba2f-7b968f340a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7186a16-4296-4fa7-a276-51a302f857f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fbbe9-6b51-4918-a3a2-a16585589313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-myenv-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
