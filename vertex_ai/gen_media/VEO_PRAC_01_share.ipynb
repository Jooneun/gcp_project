{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "293ce176-79b6-40cf-9f15-feedacae644e",
   "metadata": {},
   "source": [
    "# CE Enablement Session - Practice 01\n",
    "- ## Workflow: IMG --> Nano --> VEO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b321977-2b38-4d94-b06c-9e6deb32177d",
   "metadata": {},
   "source": [
    "### 1. (ê¶Œì¥) ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”\n",
    "python -m venv venv\n",
    "#### macOS/Linux:\n",
    "source venv/bin/activate\n",
    "\n",
    "#### 2. í•„ìˆ˜ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "pip install --upgrade google-cloud-aiplatform Pillow ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1a7540-ea61-411a-9109-7156d86ff26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ë° ì „ì—­ ì„¤ì •\n",
    "# ==============================================================================\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import mimetypes\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "# Third-party libraries\n",
    "import requests\n",
    "from PIL import Image as PIL_Image\n",
    "\n",
    "# Google Cloud and Generative AI libraries\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.genai.types import (\n",
    "    EditImageConfig,\n",
    "    GenerateImagesConfig,\n",
    "    Image,\n",
    "    MaskReferenceConfig,\n",
    "    MaskReferenceImage,\n",
    "    RawReferenceImage,\n",
    ")\n",
    "\n",
    "# IPython display for notebooks\n",
    "from IPython.display import display\n",
    "\n",
    "# --- ì „ì—­ ìƒìˆ˜ ì •ì˜ ---\n",
    "# 'jc-gcp-project'ë¥¼ ë‹¹ì‹ ì˜ GCP í”„ë¡œì íŠ¸ IDë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”.\n",
    "PROJECT_ID = \"jc-gcp-project\"\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "OUTPUT_DIR = \"veo_story_telling\"\n",
    "\n",
    "# --- ì „ì—­ ë³€ìˆ˜ ë° ì´ˆê¸°í™” ---\n",
    "# ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# API í´ë¼ì´ì–¸íŠ¸ë¥¼ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ì—¬ ëª¨ë“  í•¨ìˆ˜ì—ì„œ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# ì´ë ‡ê²Œ í•˜ë©´ ë°˜ë³µì ì¸ ì¸ì¦ ë° ê°ì²´ ìƒì„± ì˜¤ë²„í—¤ë“œê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤.\n",
    "try:\n",
    "    CLIENT = genai.Client(vertexai=True, project=PROJECT_ID, location=\"global\")\n",
    "    print(\"âœ… GenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    CLIENT = None\n",
    "\n",
    "# VEO ë¹„ë””ì˜¤ ìƒì„±ì„ ìœ„í•œ Image ë°ì´í„° í´ë˜ìŠ¤\n",
    "# í•¨ìˆ˜ ì™¸ë¶€ë¡œ ì´ë™í•˜ì—¬ ë°˜ë³µì ì¸ í´ë˜ìŠ¤ ì •ì˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "class VeoImage:\n",
    "    \"\"\"VEO APIì— ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ ë°ì´í„° í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\"\"\"\n",
    "    def __init__(self, gcs_uri=None, image_bytes=None, mime_type=None):\n",
    "        self.gcs_uri = gcs_uri\n",
    "        self.image_bytes = image_bytes\n",
    "        self.mime_type = mime_type\n",
    "\n",
    "    def __repr__(self):\n",
    "        bytes_repr = str(self.image_bytes[:60]) + '...' if self.image_bytes else None\n",
    "        return (f\"VeoImage(gcs_uri={self.gcs_uri}, \"\n",
    "                f\"mime_type='{self.mime_type}', \"\n",
    "                f\"image_bytes={bytes_repr})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb34ee7-853e-4607-9184-46c5549dda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. í—¬í¼ í•¨ìˆ˜ (Helper Functions)\n",
    "# ==============================================================================\n",
    "\n",
    "def _load_image_part(path: str) -> Optional[types.Part]:\n",
    "    \"\"\"\n",
    "    ë¡œì»¬ ê²½ë¡œ ë˜ëŠ” GCS URIì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ì—¬ genai.types.Part ê°ì²´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    ì´ í•¨ìˆ˜ëŠ” ì½”ë“œ ì¤‘ë³µì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if path.startswith(\"gs://\"):\n",
    "        mime_type, _ = mimetypes.guess_type(path)\n",
    "        if not mime_type:\n",
    "            mime_type = \"image/jpeg\"  # GCS ê¸°ë³¸ê°’\n",
    "        print(f\"âœ… GCS ì´ë¯¸ì§€ ë¡œë“œ: {path}\")\n",
    "        return types.Part.from_uri(file_uri=path, mime_type=mime_type)\n",
    "    else:\n",
    "        try:\n",
    "            mime_type, _ = mimetypes.guess_type(path)\n",
    "            if not mime_type:\n",
    "                raise IOError(f\"íŒŒì¼ì˜ MIME íƒ€ì…ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}\")\n",
    "            \n",
    "            with open(path, \"rb\") as f:\n",
    "                image_bytes = f.read()\n",
    "            \n",
    "            print(f\"âœ… ë¡œì»¬ ì´ë¯¸ì§€ ë¡œë“œ: {path}\")\n",
    "            return types.Part.from_bytes(data=image_bytes, mime_type=mime_type)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âŒ ì˜¤ë¥˜: ë¡œì»¬ íŒŒì¼ '{path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì˜¤ë¥˜: ë¡œì»¬ íŒŒì¼ '{path}' ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ: {type(e).__name__} - {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d188ef-aaaf-4c81-89b1-bb71b7c0a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. í•µì‹¬ ìƒì„± í•¨ìˆ˜ (Core Generative Functions)\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_prompt_from_images(user_prompt: str, image_paths: List[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´‘ê³ ì— ìµœì í™”ëœ ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if not CLIENT:\n",
    "        print(\"ì˜¤ë¥˜: GenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    if not image_paths:\n",
    "        print(\"ì˜¤ë¥˜: í•˜ë‚˜ ì´ìƒì˜ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    if len(image_paths) > 2:\n",
    "        print(\"ê²½ê³ : ìµœëŒ€ 2ê°œì˜ ì´ë¯¸ì§€ë§Œ ì²˜ë¦¬ë©ë‹ˆë‹¤. ì²˜ìŒ 2ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        image_paths = image_paths[:2]\n",
    "\n",
    "    try:\n",
    "        default_prompt = \"\"\"\n",
    "        You will receive a photo and a user prompt. Your task is to analyze both and generate a detailed and specific prompt that enhances the quality and relevance of the advertisement, **while explicitly prioritizing the preservation of existing subjects and objects in the original image.**\n",
    "\n",
    "        For example, if the user asks to composite a person into a photo, analyze the photo to suggest appropriate poses for the person that *integrate seamlessly with the existing scene*. If the user asks to change the text on a poster, create a prompt that ensures the new text blends seamlessly with the original design *without altering the original visual elements unnecessarily*.\n",
    "\n",
    "        The goal is to enhance the advertisement by subtly modifying or adding elements that complement the original, rather than replacing or heavily altering it.\n",
    "\n",
    "        Example Output:\"Enhance the existing image by subtly adjusting the lighting to highlight the cosmetic product, ensuring the woman's pose and expression remain the same but are visually optimized for the product's branding and target audience. Do not alter the woman's appearance or the product's original form.\"        \n",
    "       \n",
    "        The customer request is : \n",
    "        \"\"\"\n",
    "        parts = [types.Part.from_text(text=default_prompt), types.Part.from_text(text=user_prompt)]\n",
    "\n",
    "        for path in image_paths:\n",
    "            image_part = _load_image_part(path)\n",
    "            if image_part:\n",
    "                parts.append(image_part)\n",
    "\n",
    "        system_instruction_text = \"\"\"\n",
    "        You are a prompt expert specializing in advertising. Your task is to take a photo and a user prompt as input and generate a refined prompt that is optimized for creating effective advertisements.\n",
    "        \"\"\"\n",
    "\n",
    "        model = \"gemini-2.5-flash\"\n",
    "        contents = [types.Content(role=\"user\", parts=parts)]\n",
    "\n",
    "        generate_content_config = types.GenerateContentConfig(\n",
    "            temperature=1, top_p=1, seed=0, max_output_tokens=8192,\n",
    "            safety_settings=[\n",
    "                types.SafetySetting(category=c, threshold=\"BLOCK_NONE\") for c in\n",
    "                [\"HARM_CATEGORY_HATE_SPEECH\", \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"HARM_CATEGORY_HARASSMENT\"]\n",
    "            ],\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema={\"type\": \"OBJECT\", \"properties\": {\"response\": {\"type\": \"STRING\"}}},\n",
    "            system_instruction=[types.Part.from_text(text=system_instruction_text)],\n",
    "        )\n",
    "        \n",
    "        print(\"\\nìƒì„± ì¤‘...\")\n",
    "        response = CLIENT.models.generate_content(\n",
    "            model=model, contents=contents, config=generate_content_config\n",
    "        )\n",
    "        print(\"ìƒì„± ì™„ë£Œ.\")\n",
    "        \n",
    "        try:\n",
    "            parsed_json = json.loads(response.text)\n",
    "            return parsed_json.get('response')\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"âŒ ì˜¤ë¥˜: API ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ì‘ë‹µ: {response.text}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_edited_image(prompt: str, image_paths: List[str], save_image: bool = True, output_path: Optional[str] = None) -> tuple[Optional[bytes], Optional[str]]:\n",
    "    \"\"\"\n",
    "    ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³ , í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜ë  ê²½ìš° ì§€ëŠ¥ì ìœ¼ë¡œ ì¬ì‹œë„í•˜ì—¬ ê²°ê³¼ë¬¼ì„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if not CLIENT:\n",
    "        print(\"ì˜¤ë¥˜: GenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return None, None\n",
    "\n",
    "    user_parts = []\n",
    "    for path in image_paths:\n",
    "        image_part = _load_image_part(path)\n",
    "        if image_part:\n",
    "            user_parts.append(image_part)\n",
    "    \n",
    "    user_parts.append(types.Part.from_text(text=prompt))\n",
    "    initial_contents = [types.Content(role=\"user\", parts=user_parts)]\n",
    "\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        temperature=1, top_p=0.95, max_output_tokens=32768,\n",
    "        response_modalities=[\"TEXT\", \"IMAGE\"],\n",
    "        safety_settings=[\n",
    "            types.SafetySetting(category=c, threshold=\"BLOCK_NONE\") for c in\n",
    "            [\"HARM_CATEGORY_HATE_SPEECH\", \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"HARM_CATEGORY_HARASSMENT\"]\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    image_found = False\n",
    "    model_response_text = \"\"\n",
    "    received_image_bytes = b\"\"\n",
    "    final_path = None\n",
    "\n",
    "    def process_stream_response(current_contents):\n",
    "        nonlocal image_found, model_response_text, received_image_bytes, final_path\n",
    "        \n",
    "        for chunk in CLIENT.models.generate_content_stream(\n",
    "            model=\"gemini-2.5-flash-image-preview\", contents=current_contents, config=generate_content_config,\n",
    "        ):\n",
    "            for part in chunk.candidates[0].content.parts:\n",
    "                if part.text:\n",
    "                    model_response_text += part.text\n",
    "                    print(part.text, end=\"\")\n",
    "                \n",
    "                if part.inline_data:\n",
    "                    image_found = True\n",
    "                    received_image_bytes = part.inline_data.data\n",
    "                    print(\"\\n\\n--- âœ… ì´ë¯¸ì§€ ìˆ˜ì‹  ì™„ë£Œ ---\")\n",
    "                    \n",
    "                    if save_image:\n",
    "                        if output_path:\n",
    "                            final_path = output_path\n",
    "                        else:\n",
    "                            default_dir = os.path.join(OUTPUT_DIR, \"sample_imgs\")\n",
    "                            os.makedirs(default_dir, exist_ok=True)\n",
    "                            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                            filename = f\"output_{timestamp}.png\"\n",
    "                            final_path = os.path.join(default_dir, filename)\n",
    "                        \n",
    "                        try:\n",
    "                            with open(final_path, \"wb\") as f:\n",
    "                                f.write(received_image_bytes)\n",
    "                            print(f\"ğŸ’¾ ì´ë¯¸ì§€ê°€ '{os.path.abspath(final_path)}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "                        except IOError as e:\n",
    "                            print(f\"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    \n",
    "    print(f\"--- 1ì°¨ ì´ë¯¸ì§€ ìƒì„± ì‹œë„ (ì…ë ¥ ì´ë¯¸ì§€ {len(image_paths)}ê°œ) ---\")\n",
    "    process_stream_response(initial_contents)\n",
    "    \n",
    "    if not image_found:\n",
    "        print(\"\\n\\n--- âš ï¸ 1ì°¨ ì‹œë„ì—ì„œ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 2ì°¨ ì‹œë„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ---\")\n",
    "        \n",
    "        retry_contents = list(initial_contents)\n",
    "        retry_contents.append(types.Content(role=\"model\", parts=[types.Part.from_text(text=model_response_text)]))\n",
    "        retry_contents.append(types.Content(role=\"user\", parts=[types.Part.from_text(text=\"ë„¤, ì¢‹ìŠµë‹ˆë‹¤. ì–´ë–¤ ë°©ë²•ì„ ì¶”ê°€í•´ì„œë¼ë„ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.\")]))\n",
    "\n",
    "        image_found = False\n",
    "        model_response_text = \"\"\n",
    "        received_image_bytes = b\"\"\n",
    "\n",
    "        process_stream_response(retry_contents)\n",
    "        print(\"\\n\\n--- âœ… 2ì°¨ ì‹œë„ ì™„ë£Œ ---\")\n",
    "\n",
    "    if not image_found:\n",
    "        print(\"\\n\\n--- âŒ ìµœì¢…ì ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ---\")\n",
    "    \n",
    "    return received_image_bytes, final_path\n",
    "\n",
    "\n",
    "def generate_video_from_image(image_path: str, scene_story: str, scene_idx: int) -> str:\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ì´ë¯¸ì§€ì™€ ìŠ¤í† ë¦¬ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    if not CLIENT:\n",
    "        print(\"ì˜¤ë¥˜: GenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        mime_type, _ = mimetypes.guess_type(image_path)\n",
    "        if not mime_type:\n",
    "            raise ValueError(f\"Could not determine MIME type for {image_path}\")\n",
    "\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            image_byte_data = image_file.read()\n",
    "\n",
    "        image_instance = VeoImage(\n",
    "            image_bytes=image_byte_data,\n",
    "            mime_type=mime_type\n",
    "        )\n",
    "        print(\"âœ… VeoImage ê°ì²´ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ '{image_path}'\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    prompt = f\"\"\"Generate a video from this image that shows: {scene_story}\n",
    "    The video should be smooth and cinematic, lasting 8 seconds.\n",
    "    Create natural movement and progression from the starting image.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        operation = CLIENT.models.generate_videos(\n",
    "            model=\"veo-3.0-generate-preview\",\n",
    "            prompt=prompt,\n",
    "            image=image_instance,\n",
    "        )\n",
    "\n",
    "        while not operation.done:\n",
    "            print(\"ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...\")\n",
    "            time.sleep(10)\n",
    "            operation = CLIENT.operations.get(operation)\n",
    "\n",
    "        if operation.done and not operation.error:\n",
    "            video_bytes = operation.response.generated_videos[0].video.video_bytes\n",
    "            video_path = f\"{OUTPUT_DIR}/scene_{scene_idx}_video.mp4\"\n",
    "            with open(video_path, 'wb') as f:\n",
    "                f.write(video_bytes)\n",
    "            print(f\"'{video_path}' íŒŒì¼ë¡œ ë¹„ë””ì˜¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "            return video_path\n",
    "        else:\n",
    "            print(\"ë¹„ë””ì˜¤ ìƒì„± ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            if operation.error:\n",
    "                print(f\"ì˜¤ë¥˜: {operation.error}\")\n",
    "            return \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë¹„ë””ì˜¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d994864-14c1-4862-a0e0-9c7039cb5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. ì´ë¯¸ì§€ í¸ì§‘/ì•„ì›ƒí˜ì¸íŒ… í•¨ìˆ˜\n",
    "# ==============================================================================\n",
    "\n",
    "def get_bytes_from_pil(image: PIL_Image.Image) -> bytes:\n",
    "    \"\"\"PIL ì´ë¯¸ì§€ë¥¼ PNG ë°”ì´íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    byte_io_png = io.BytesIO()\n",
    "    image.save(byte_io_png, \"PNG\")\n",
    "    return byte_io_png.getvalue()\n",
    "\n",
    "def pad_to_target_size(source_image, target_size=(1536, 1536), mode=\"RGB\", vertical_offset_ratio=0, horizontal_offset_ratio=0, fill_val=255):\n",
    "    \"\"\"ì•„ì›ƒí˜ì¸íŒ…ì„ ìœ„í•´ ì´ë¯¸ì§€ë¥¼ íŠ¹ì • í¬ê¸°ë¡œ íŒ¨ë”©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    orig_w, orig_h = source_image.size\n",
    "    target_w, target_h = target_size\n",
    "    \n",
    "    insert_x = int((target_w - orig_w) / 2 + horizontal_offset_ratio * target_w)\n",
    "    insert_y = int((target_h - orig_h) / 2 + vertical_offset_ratio * target_h)\n",
    "    insert_x = min(max(0, insert_x), target_w - orig_w)\n",
    "    insert_y = min(max(0, insert_y), target_h - orig_h)\n",
    "    \n",
    "    color = (fill_val, fill_val, fill_val) if mode == \"RGB\" else fill_val\n",
    "    padded_image = PIL_Image.new(mode, target_size, color=color)\n",
    "    padded_image.paste(source_image, (insert_x, insert_y))\n",
    "    return padded_image\n",
    "\n",
    "def pad_image_and_mask(image_pil, mask_pil, target_size, v_offset, h_offset):\n",
    "    \"\"\"ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ë¥¼ ëŒ€ìƒ í¬ê¸°ì— ë§ê²Œ ì¡°ì •í•˜ê³  íŒ¨ë”©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    image_pil.thumbnail(target_size)\n",
    "    mask_pil.thumbnail(target_size)\n",
    "    \n",
    "    image_pil = pad_to_target_size(image_pil, target_size, \"RGB\", v_offset, h_offset, 0)\n",
    "    mask_pil = pad_to_target_size(mask_pil, target_size, \"L\", v_offset, h_offset, 255)\n",
    "    return image_pil, mask_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "969715de-3cbc-40da-b635-d4a0303c063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. ë©”ì¸ ì‹¤í–‰ ë¡œì§\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e39ca2-65fd-4084-b6fc-977071f1c35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ğŸš€ 1ë‹¨ê³„: ê´‘ê³  ì¹´í”¼ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± ---\n",
      "âœ… ë¡œì»¬ ì´ë¯¸ì§€ ë¡œë“œ: veo_story_telling/sample_imgs/two_people_ads.png\n",
      "\n",
      "ìƒì„± ì¤‘...\n",
      "ìƒì„± ì™„ë£Œ.\n",
      "Identify the Korean text \"ë¶€ë“œëŸ¬ì›€ê³¼ ê°ê°ì ì¸ í–¥ ì–´ë…¸ë¸Œë¡œ ì™„ì„±í•˜ëŠ” ëŒ€ë‹´í•œ ìš°ì•„í•¨\" in the image. Replace this specific Korean text with the English phrase \"A bold elegance perfected with soft and sensual ANOVE fragrance.\". Ensure that all other elements of the original image, including the models, background, lighting, and the existing English text \"UNLEASH YOUR\" and \"Bold Elegance\", remain completely unchanged. The new English text should seamlessly adopt the font style, color, size, and spatial alignment of the original Korean text to maintain visual consistency.\n"
     ]
    }
   ],
   "source": [
    "# --- 1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„± ---\n",
    "print(\"--- ğŸš€ 1ë‹¨ê³„: ê´‘ê³  ì¹´í”¼ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± ---\")\n",
    "prompt_text = \"ê´‘ê³  ì´ë¯¸ì§€ ì† í•œê¸€ì„ ì˜ ì–´ìš¸ë¦¬ëŠ” ì˜ì–´ë¡œ ë³€ê²½í•˜ê³  ì‹¶ì–´.\"\n",
    "image_files = [\"veo_story_telling/sample_imgs/two_people_ads.png\"]\n",
    "# ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë³€ìˆ˜ì— ì €ì¥\n",
    "generated_prompt = generate_prompt_from_images(user_prompt=prompt_text, image_paths=image_files)\n",
    "print(generated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6cdd0c-8126-4a90-916f-c0b20b41963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„± ---\n",
    "# print(\"--- ğŸš€ 1ë‹¨ê³„: ê´‘ê³  ì¹´í”¼ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± ---\")\n",
    "# prompt_text = \"ì²«ë²ˆì§¸ ì‚¬ì§„ ì† ëª¨ë¸ì´ ë‘ë²ˆì§¸ ì œí’ˆì„ ê´‘ê³ í•˜ëŠ” ì‚¬ì§„ìœ¼ë¡œ ë§Œë“¤ê³  ì‹¶ì–´. ìµœëŒ€í•œ ìì—°ìŠ¤ëŸ½ê²Œ ì œí’ˆ ì‚¬ìš©í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë§Œë“¤ê³ , ì´ë¯¸ì§€ì† ê¸€ìëŠ”ì§€ì›Œ\"\n",
    "# image_files = [\"veo_story_telling/sample_imgs/rose.png\", \"veo_story_telling/sample_imgs/product_02_cosme.png\"]\n",
    "# # ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë³€ìˆ˜ì— ì €ì¥\n",
    "# generated_prompt = generate_prompt_from_images(user_prompt=prompt_text, image_paths=image_files)\n",
    "# print(generated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2eca70-66af-4f0f-9310-911f1cc44baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2ë‹¨ê³„: ì´ë¯¸ì§€ í¸ì§‘ ---\n",
    "print(\"\\n--- ğŸš€ 2ë‹¨ê³„: ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ í¸ì§‘ ---\")\n",
    "edited_image_bytes, edited_image_path = generate_edited_image(\n",
    "    prompt=generated_prompt,\n",
    "    image_paths=image_files\n",
    ")\n",
    "\n",
    "print(\"\\n--- âœ… í¸ì§‘ëœ ì´ë¯¸ì§€ ---\")\n",
    "display(PIL_Image.open(io.BytesIO(edited_image_bytes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94b73c-13ef-4caa-aeb0-398f0dd5f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3ë‹¨ê³„: ì•„ì›ƒí˜ì¸íŒ… ---\n",
    "print(\"\\n--- ğŸš€ 3ë‹¨ê³„: í¸ì§‘ëœ ì´ë¯¸ì§€ 16:9 ë¹„ìœ¨ë¡œ ì•„ì›ƒí˜ì¸íŒ… ---\")\n",
    "\n",
    "initial_image_pil = PIL_Image.open(edited_image_path)\n",
    "mask_pil = PIL_Image.new(\"L\", initial_image_pil.size, 0)\n",
    "\n",
    "image_height = 500\n",
    "image_width = int(image_height * 16 / 9)\n",
    "target_size = (image_width, image_height)\n",
    "\n",
    "image_pil_outpaint, mask_pil_outpaint = pad_image_and_mask(\n",
    "    initial_image_pil, mask_pil, target_size, 0, 0\n",
    ")\n",
    "\n",
    "image_pil_outpaint_image = Image(image_bytes=get_bytes_from_pil(image_pil_outpaint))\n",
    "mask_pil_outpaint_image = Image(image_bytes=get_bytes_from_pil(mask_pil_outpaint))\n",
    "\n",
    "raw_ref_image = RawReferenceImage(reference_image=image_pil_outpaint_image, reference_id=0)\n",
    "mask_ref_image = MaskReferenceImage(\n",
    "    reference_id=1,\n",
    "    reference_image=mask_pil_outpaint_image,\n",
    "    config=MaskReferenceConfig(mask_mode=\"MASK_MODE_USER_PROVIDED\", mask_dilation=0.03),\n",
    ")\n",
    "\n",
    "outpaint_prompt = \"\"\"\n",
    "This image is a crop of a much larger photograph.\n",
    "Reveal the rest of the original scene.\n",
    "Extend the environment naturally, maintaining the exact same lighting, mood, and fine details as the provided image.\n",
    "\"\"\"\n",
    "outpainted_result = CLIENT.models.edit_image(\n",
    "    model=\"imagen-3.0-capability-001\",\n",
    "    prompt=outpaint_prompt,\n",
    "    reference_images=[raw_ref_image, mask_ref_image],\n",
    "    config=EditImageConfig(\n",
    "        edit_mode=\"EDIT_MODE_OUTPAINT\", number_of_images=1,\n",
    "        safety_filter_level=\"BLOCK_MEDIUM_AND_ABOVE\", person_generation=\"ALLOW_ADULT\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "generated_image_bytes = outpainted_result.generated_images[0].image.image_bytes\n",
    "output_filename = os.path.join(OUTPUT_DIR, \"sample_imgs\", \"edited_image_result.png\")\n",
    "\n",
    "with open(output_filename, \"wb\") as f:\n",
    "    f.write(generated_image_bytes)\n",
    "print(f\"âœ… ì•„ì›ƒí˜ì¸íŒ… ì´ë¯¸ì§€ê°€ '{output_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\n--- âœ… ì•„ì›ƒí˜ì¸íŒ… ê²°ê³¼ ì´ë¯¸ì§€ ---\")\n",
    "display(PIL_Image.open(io.BytesIO(generated_image_bytes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4df6f9b-a6fa-47e0-8822-b5e5d805816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VeoImage ê°ì²´ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...\n",
      "ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...\n",
      "ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...\n",
      "ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...\n",
      "ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...\n",
      "ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...\n",
      "'veo_story_telling/scene_1_video.mp4' íŒŒì¼ë¡œ ë¹„ë””ì˜¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "input_img_file = \"veo_story_telling/sample_imgs/edited_image_result.png\"\n",
    "veo_prompt = \"ì‚¬ì§„ì† ë‘ ëª¨ë¸ì´ ìì—°ìŠ¤ëŸ¬ìš´ í¬ì¦ˆë¥¼ ì·¨í•˜ëŠ” ì˜ìƒ\"\n",
    "\n",
    "vid_result = generate_video_from_image(input_img_file, veo_prompt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575989a-a8ad-4f60-8151-3adb08c0d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_img_file = \"veo_story_telling/sample_imgs/edited_image_result.png\"\n",
    "# veo_prompt = \"í™”ì¥í’ˆì„ ê´‘ê³ í•˜ê¸° ìœ„í•´ íŒ”ì— ë¬¸ì§€ë¥´ëŠ” ì˜ìƒ\"\n",
    "\n",
    "# vid_result = generate_video_from_image(input_img_file, veo_prompt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d09041-83a5-4bb9-97dc-779f6cadc2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-myenv-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
