# -*- coding: utf-8 -*-
"""jc_sol_ply_vs_edit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iVeMfDIydTM2tJLsz5ZhTfTBTOAA5_un
"""

import os
import pandas as pd
import numpy as np
import copy
from datetime import datetime, timedelta
from airflow import DAG
from airflow.decorators import task, dag
from airflow.operators.dummy import DummyOperator
from airflow.operators.bash_operator import BashOperator
from airflow.providers.google.cloud.operators.dataproc import DataprocSubmitJobOperator
from airflow.contrib.operators.bigquery_operator import BigQueryOperator
from airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryExecuteQueryOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from airflow.decorators import task, dag
from airflow.models.baseoperator import chain
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from airflow.models import Variable


with DAG(
    'Joone_sol_ply',
    # These args will get passed on to each operator
    # You can override them on a per-task basis during operator initialization
    default_args={
        'depends_on_past': False,
        'email': ['airflow@example.com'],
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
        # 'queue': 'bash_queue',
        # 'pool': 'backfill',
        # 'priority_weight': 10,
        # 'end_date': datetime(2016, 1, 1),
        # 'wait_for_downstream': False,
        # 'sla': timedelta(hours=2),
        # 'execution_timeout': timedelta(seconds=300),
        # 'on_failure_callback': some_function,
        # 'on_success_callback': some_other_function,
        # 'on_retry_callback': another_function,
        # 'sla_miss_callback': yet_another_function,
        # 'trigger_rule': 'all_success'
    },
    description='A Solution Play tutorial DAG',
    schedule_interval=timedelta(days=1),
    start_date=datetime.combine(datetime.today() - timedelta(1), datetime.min.time()),
    catchup=False,
    tags=['jc_solution_play'],
) as dag:
    # Get Variables
    vars = Variable.get(key='global_vars', deserialize_json=True)
    # value1 = json['value_1'] ν™•μΈν•„μ”
    project_id = vars['project_id']
    dataset_name = vars['dataset_name']
    bucket_id = vars['bucket_id']
    csv_route_in_bucket = vars['csv_route_in_bucket']
    cluster_name = vars['cluster_name']
    region = vars['region']
    job_file_uri = vars['job_file_uri']
    default_sql = vars['default_sql']


    ### GCS --> BQ
    start = DummyOperator(task_id = "Start")

    t1 = GCSToBigQueryOperator(
        task_id="get_raw_data",
        bucket=bucket_id,
        source_objects=[csv_route_in_bucket],
        destination_project_dataset_table=f"{project_id}.{dataset_name}.raw_data",
        autodetect=True,
        write_disposition="WRITE_TRUNCATE",
      )

    #@@ Preprocessing
    query_preprocessing = f"{default_sql}"
    t2 = BigQueryOperator(
        task_id='prepreprocessing',
        sql=query_preprocessing,
        use_legacy_sql=False,
        destination_dataset_table=f"{project_id}.{dataset_name}.preprocessed_data",
        write_disposition='WRITE_TRUNCATE'
      )


    PYSPARK_JOB = {
        "reference": {"project_id": project_id},
        "placement": {"cluster_name": cluster_name},
        "pyspark_job": {"main_python_file_uri": job_file_uri},
    }

    t3 = DataprocSubmitJobOperator(
        task_id="data_sementation", job=PYSPARK_JOB, region=region, project_id=project_id
      )


    query_vector_idx = f"CREATE OR REPLACE VECTOR INDEX embed_idx ON {dataset_name}.vectorized_data(vectors) OPTIONS(distance_type='COSINE', index_type='IVF')"
    t4 = BigQueryOperator(
        task_id='vector_index',
        sql=query_vector_idx,
        use_legacy_sql=False,
        write_disposition='WRITE_TRUNCATE'
      )
####

    query_output = f"EXPORT DATA OPTIONS (uri = 'gs://{bucket_id}/data/output_*.json', format = 'JSON', overwrite = true) AS (SELECT * FROM `{project_id}.{dataset_name}.vectorized_data`)"
    t5 = BigQueryOperator(
        task_id='get_output',
        sql=query_output,
        use_legacy_sql=False,
        write_disposition='WRITE_TRUNCATE'
      )

####
    end = DummyOperator(task_id = "End")

    start >> t1 >> t2 >> t3 >> t4 >> t5 >> end