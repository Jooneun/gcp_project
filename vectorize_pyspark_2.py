# -*- coding: utf-8 -*-
"""vectorize_pyspark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E8NZSzGYagrpsJgWmkMBch8f5_GNuIh8
"""

import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from pyspark.sql import Window

spark = SparkSession.builder \
  .config("spark.jars.packages", "com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.36.1,hadoop-gcs-connector-3.4.0.jar") \
  .config("temporaryGcsBucket", "gs://us-central1-jc-solution-pla-70d0796d-bucket") \
  .config("fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
  .config("fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS") \
  .getOrCreate()

#### 확정
df_preprocessed = spark.read.format("bigquery") \
  .option('table', "jc-gcp-project:vector_search1.preprocessed_data") \
  .load()

window_size = 20  # Adjust as needed
step_size = 1  # Adjust as needed

windowSpec = F.window("date", str(window_size) + " days", str(step_size) + " days")

# def process_item_group(item_df):
#     """item_df: 각 item_name에 대한 그룹 데이터"""

#     # 슬라이딩 윈도우 생성 (날짜로 정렬 필요)
#     w = Window.partitionBy("item_name").orderBy("date").rowsBetween(-window_size + 1, 0)

#     return item_df.withColumn("start_time", F.first("date").over(w)) \
#                   .withColumn("end_time", F.last("date").over(w)) \
#                   .withColumn("vectors", F.collect_list("total_amount_sold").over(w)) \
#                   .select("start_time", "end_time", "item_name", "vectors")

def process_item_group(item_df):
    """item_df: 각 item_name에 대한 그룹 데이터"""

    # 슬라이딩 윈도우 생성 (날짜로 정렬 필요)
    w = Window.partitionBy("item_name").orderBy("date").rowsBetween(-window_size + 1, 0)

    return item_df.withColumn("start_time", F.first("date").over(w)) \
                  .withColumn("end_time", F.last("date").over(w)) \
                  .withColumn("vectors", F.collect_list(F.col("total_amount_sold").cast("float")).over(w)) \
                  .filter(F.size("vectors") >= window_size) \
                  .select("start_time", "end_time", "item_name", "vectors")


result_dfs = []  # 중간 결과 DataFrame 저장

# item_name 별 그룹 처리
sum_cnt = 0 # 삭제 필요
for item in df_preprocessed.select("item_name").distinct().rdd.flatMap(lambda x: x).collect():
    item_df = df_preprocessed.filter(F.col("item_name") == item)
    sum_cnt+=1 # 삭제 필요
    if item_df.count() > window_size:
        result_dfs.append(process_item_group(item_df))
        if sum_cnt > 100: # 삭제 필요
            break
# 마지막 결합
embedding_vec = result_dfs[0]
for i in range(1, len(result_dfs)):
    embedding_vec = embedding_vec.union(result_dfs[i])

embedding_vec.show()

embedding_vec.write \
  .format("bigquery") \
  .option("writeMethod", "direct") \
  .mode("overwrite") \
  .save("jc-gcp-project:vector_search1.vectorized_data")